# from selenium import webdriver
#
# browser = webdriver.Firefox(executable_path=r"C:\bkfs_legal\web_scrapping\geckodriver.exe")
# browser.get('https://publicrecords.netronline.com/')
# browser.find_element_by_name('locator-city').send_keys('MN')
# browser.find_elements_by_class_name('cfind ui-button-success ui-button ui-widget ui-state-default ui-corner-all').click()
#
#
#
#
import pprint
import requests
from bs4 import BeautifulSoup
from selenium.webdriver import Firefox
from selenium.webdriver.firefox.options import Options
import pandas
# from selenium.webdriver.support.select import
opts = Options()
opts.set_headless()
assert opts.headless  # Operating in headless mode
browser = Firefox(executable_path=r"C:\bkfs_legal\web_scrapping\geckodriver.exe",options=opts)
browser.get('http://www.whitesidecountyassessor.com/propertysearch/default.html')
# search_form = browser.find_element_by_name('locator-city')
# # search_form.send_keys('IL')
# select = Select(browser.find_element_by_name('locator-state'))

# select by visible text
# select.select_by_visible_text('Banana')

# select by value
# select.select_by_value('IL')
browser.find_element_by_name('Button1').click()
browser.get('http://www.whitesidecountyassessor.com/propertysearch/pinsearch.aspx')
browser.find_element_by_id('Pin1').send_keys('1119251012')
browser.find_element_by_name('Search').click()
browser.find_element_by_partial_link_text('11-19-251-012').click()
# search_form = browser.find_element_by_link_text('records.php?county=690&state=15')
parcel = '11-19-252-005'
source_page = requests.get('http://www.whitesidecountyassessor.com/propertysearch/detail.aspx?Mpin='+parcel)
# SOURCE NAME EXTRACTION
soup = BeautifulSoup(source_page.content, 'html.parser')
# Headers Data
data = soup.find('table',{"id" : "FormView1_GridView2"}).find_all('th',{"scope" : "col"})
# To get the headers for the tabular data
first_row_data = data
# print(first_row_data)
header_list = []
for row in first_row_data:
    header_list.append(row.text)
print(header_list)

# TO get the data for the headers
data_list = []
headers_data = soup.find('table',{"id" : "FormView1_GridView2"}).find_all('td')
new_data = headers_data
for col in new_data:
    data_list.append(col.text)
print(data_list)

#
# row_data = soup.find('table',{"id" : "FormView1_GridView2"}).find_all('tr')
# temp = row_data

# for tmp in range(0,len(first_row_data),8):
#     yield data_list[tmp:tmp+8]



    # How many elements each


# list should have

#
# x = list(divide_chunks(data_list, 8))
# print(x)
# samp_list = []
# for samp in temp:
#     samp_list.append(samp.text)
#
# print(samp_list)
#
# for headers,content in zip(first_row_data,headers_data):
#     print(headers.text, " ",content.text)
# temp = soup.find_all(class_='auto-style1')
# new_list = []
# state = ''
# new_row = ''
# temp = soup.find_all('a')
# for x in temp:
#     new_tag = soup.find_all('a')
#     for row in new_tag[1]:
#         new_row = str(row).lstrip('\t')
#         new_list.append(new_row)
# for col in new_list:
#     print(col)
#     state = str(new_tag[0]).split(">")
# print(state[1].rstrip('\t\n'))

